{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div>\n  <hr>\n  <h1 align=\"middle\">Проект по реализации <b>генеративной модели (GAN)</b>.</h1>\n  <ul>\n    <li><h4>Выполнил: Мартынов Владислав</h4></li>\n    <li><h4>Поток: Продвинутый</h4></li>\n    <li><h4>StepikID: 596247708</h4></li>\n    <li><h4>GitHub: <a href=\"\"https://github.com/VladMartinov>VladMartinov</a></h4></li>\n  </ul>\n  <hr>\n  <h2>План реализации проекта:</h2>\n  <ol>\n    <li><h4>Определение задачи и выбор архитектуры;</h4></li>\n    <li><h4>Тестирование <b>своей</b> модели на уже решенной задаче;</h4></li>\n    <li><h4>Поиск dataset'а для своей задачи;</h4></li>\n    <li><h4>Решение поставленной задачи (своей) при помощи своей модели и своего dataset'а;</h4></li>\n    <li><h4>Реализация удобного интерфейса для генерации.</h4></li>\n  </ol>\n  <hr>\n  <h2>План реализации модели:</h2>\n  <ol>\n    <li><h4>Изучение структур различных генеративных моделей типа CycleGan;</h4></li>\n    <li><h4>Реализация моделей и их тестирование;</h4></li>\n  </ol>\n  <hr>\n</div>\n<img src=\"https://files.realpython.com/media/An-Introduction-to-Generative-Adversarial-Networks-GANs_Watermarked.6b71bfd66fda.jpg\" alt=\"GANS img\" align=\"middle\" />","metadata":{"id":"LHy5dSfPUxUF"}},{"cell_type":"markdown","source":"<hr>\n<h2>Инициализируем все <b>необходимые библиотеки</b> для данного проекта:</h2>","metadata":{"id":"6z3xq5BZ4Uxa"}},{"cell_type":"markdown","source":"<p>Ниже будут инициализированы весы модели (в случае если они у нас есть), подключены все необходимые импорты, а так же будут инициализированы различные параметры обучения и вспомогательные функции.</p>","metadata":{}},{"cell_type":"code","source":"!mkdir \"saved_imgs\"","metadata":{"id":"4uKkUYd_zkA7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.modules.linear import Identity\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image\n\n# Transform\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Other\nfrom tqdm import tqdm\nfrom PIL import Image\nimport os\nimport numpy as np\nimport random\nimport copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration to model training\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Path to dataset\nTRAIN_DIR = \"/kaggle/input/cyclegan-small/datasets/monet2photo/monet2photo/train\"\nVAL_DIR = \"/kaggle/input/cyclegan-small/datasets/monet2photo/monet2photo/val\"\n\nBATCH_SIZE = 1\n\nLEARNING_RATE = 1e-5\n\nLAMBDA_IDENTITY = 5\nLAMBDA_CYCLE = 10\n\nNUM_WORKERS = 2\nNUM_EPOCHS = 20\n\nLOAD_MODEL= True\nSAVE_MODEL = True\n\nCHECKPOINT_GEN_M = \"genm.pth.tar\"\nCHECKPOINT_GEN_P = \"genp.pth.tar\"\nCHECKPOINT_DISC_M = \"discm.pth.tar\"\nCHECKPOINT_DISC_P = \"discp.pth.tar\"\n\ntransforms = A.Compose(\n    [\n        A.Resize(width=256, height=256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n        ToTensorV2(),\n    ],\n    additional_targets={\"image0\": \"image\"},\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility functions\ndef save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2>Загрузка <b>dataset's</b> для тренировки моделей</h2>\n<h3>Всего будет загружено 2-а dataset'а:</h3>\n<ul>\n  <li><h4>Monet2Photo;</h4></li>\n  <li><h4>Anime2Photo (Coming soon);</h4></li>\n</ul>","metadata":{"id":"JpgOPkp8awmm"}},{"cell_type":"code","source":"if LOAD_MODEL:\n    # Path to models weight files\n    !cp /kaggle/input/cyclegan-small/discm.pth.tar /kaggle/working/\n    !cp /kaggle/input/cyclegan-small/discp.pth.tar /kaggle/working/\n    !cp /kaggle/input/cyclegan-small/genm.pth.tar /kaggle/working/\n    !cp /kaggle/input/cyclegan-small/genp.pth.tar /kaggle/working/","metadata":{"executionInfo":{"elapsed":7141,"status":"ok","timestamp":1688784913616,"user":{"displayName":"Vlad Martinov","userId":"12645653189845640665"},"user_tz":-180},"id":"AQA0Pp09bn5i","outputId":"18a5f73f-d693-4de2-9340-555f64531860","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MonetPhotoSet(Dataset):\n  def __init__(self, root_monet, root_photo, transform=None):\n    self.root_monet = root_monet\n    self.root_photo = root_photo\n\n    self.transform = transform\n\n    self.monet_imgs = os.listdir(self.root_monet)\n    self.photo_imgs = os.listdir(self.root_photo)\n\n    # Max length of this dataset's (they are not equals)\n    self.length_dataset = max(len(self.monet_imgs), len(self.photo_imgs))\n\n    self.monet_dataset_len = len(self.monet_imgs)\n    self.photo_dataset_len = len(self.photo_imgs)\n\n  def __len__(self):\n    return self.length_dataset\n\n  def __getitem__(self, index):\n    monet_img = self.monet_imgs[index % self.monet_dataset_len]\n    photo_img = self.photo_imgs[index % self.photo_dataset_len]\n\n    monet_path = os.path.join(self.root_monet, monet_img)\n    photo_path = os.path.join(self.root_photo, photo_img)\n\n    monet_img = np.array(Image.open(monet_path).convert(\"RGB\"))\n    photo_img = np.array(Image.open(photo_path).convert(\"RGB\"))\n\n    if self.transform:\n      aug = self.transform(image=monet_img, image0=photo_img)\n\n      monet_img = aug['image']\n      photo_img = aug['image0']\n\n    return monet_img, photo_img\n","metadata":{"id":"S1aIkSp2bLkH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2><b>Реализация архитектуры модели CycleGan</b></h2>","metadata":{"id":"Du7g3mhGsAXX"}},{"cell_type":"markdown","source":"<hr>\n<h2>Для <b>generato'а</b> была реализована следующая структура:</h2>","metadata":{"id":"kIVmTWNomNG0"}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n  def __init__(self, input_chanels, out_chanels, is_down=True, is_act=True, **kwargs):\n    super().__init__()\n\n    self.conv_block = nn.Sequential(\n        nn.Conv2d(input_chanels, out_chanels, padding_mode=\"reflect\", **kwargs)\n        if is_down\n        else nn.ConvTranspose2d(input_chanels, out_chanels, **kwargs),\n        nn.ReLU(inplace=True) if is_act else nn.Identity(),\n    )\n\n  def forward(self, x):\n    return self.conv_block(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, chanels):\n      super().__init__()\n\n      self.res_block = nn.Sequential(\n          ConvBlock(chanels, chanels, kernel_size=3, padding=1),\n          ConvBlock(chanels, chanels, is_act=False, kernel_size=3, padding=1),\n      )\n\n    def forward(self, x):\n      return x + self.res_block(x)\n\n\n# GAN Generator #\nclass Generator(nn.Module):\n    def __init__(self, input_channels=3, num_features=[64, 128, 256, 512], num_residual_blocks=9):\n        super().__init__()\n        #---------------------------------------------------\n        # We create the generator with the next arhitecture:\n        # Conv2d -> ReLU\n        # 2 * ConvBlock(down)\n        # num_residual_blocks * ResidualBlock\n        # 2 * ConvBlock(up)\n        # Conv2d -> tanh\n        #---------------------------------------------------\n\n        # Convolution Layears\n        self.initial_blocks = nn.Sequential(\n            nn.Conv2d(input_channels, num_features[0], kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n            nn.ReLU(inplace=True),\n        )\n\n        self.down_blocks = nn.ModuleList(\n            [\n                ConvBlock(num_features[0], num_features[1], kernel_size=3, stride=2, padding=1),\n                ConvBlock(num_features[1], num_features[2], kernel_size=3, stride=2, padding=1),\n            ]\n        )\n\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_features[2]) for _ in range(num_residual_blocks)]\n        )\n\n        self.up_blocks = nn.ModuleList(\n            [\n                ConvBlock(num_features[2], num_features[1], is_down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n                ConvBlock(num_features[1], num_features[0], is_down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n            ]\n        )\n\n        self.last_layer = nn.Conv2d(num_features[0], input_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n\n    def forward(self, x):\n        x = self.initial_blocks(x)\n\n        for layer in self.down_blocks:\n          x = layer(x)\n\n        for layer in self.res_blocks:\n          x = layer(x)\n\n        for layer in self.up_blocks:\n          x = layer(x)\n\n        x = self.last_layer(x)\n\n        return torch.tanh(x)","metadata":{"id":"Z7TLoNbUUmXN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the Generator\ndef test_G():\n  img_chanel = 3\n  img_size = 256\n\n  x = torch.randn(2, img_chanel, img_size, img_size)\n  model = Generator()\n  preds = model(x)\n\n  print(preds.shape)\n\ntest_G()","metadata":{"executionInfo":{"elapsed":3325,"status":"ok","timestamp":1688784916937,"user":{"displayName":"Vlad Martinov","userId":"12645653189845640665"},"user_tz":-180},"id":"JcK_PbqHhI5d","outputId":"837fcf7a-a21c-45b1-ba90-08d1c6b98d11","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2>Для <b>discriminator'а</b> была реализована следующая структура:</h2>","metadata":{"id":"lAqNmMwTmQwS"}},{"cell_type":"code","source":"# GAN Discriminator #\nclass InstanceBlock(nn.Module):\n    def __init__(self, input_channels, out_channels, stride):\n      super().__init__()\n\n      self.conv = nn.Sequential(\n          nn.Conv2d(input_channels, out_channels, 4, stride, 1, bias=True, padding_mode=\"reflect\"),\n          nn.InstanceNorm2d(out_channels),\n          nn.LeakyReLU(0.2),\n      )\n\n    def forward(self, x):\n      return self.conv(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels=3, num_features=[64, 128, 256, 512]):\n      super().__init__()\n      # ------------------------------------------------------\n      # We create the discriminator with the next arhitecture:\n      # Conv2d -> LeakyRely\n      # n_layers * (Conv2d -> InstanceNorm2d -> LeakyRelu)\n      # Conv2d -> sigmoid\n      # ------------------------------------------------------\n\n      self.initial_layer = nn.Sequential(\n          nn.Conv2d(input_channels, num_features[0], 4, 2, 1, padding_mode=\"reflect\"),\n          nn.LeakyReLU(0.2),\n      )\n\n      # layers array\n      layers = []\n\n      input_channels = num_features[0]\n      for feature in num_features[1:]:\n        layers.append(InstanceBlock(input_channels, feature, stride=1 if feature==num_features[-1] else 2))\n        input_channels = feature\n\n      layers.append(nn.Conv2d(input_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n      self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n      x = self.initial_layer(x)\n      return torch.sigmoid(self.model(x))","metadata":{"id":"1tqiWvLZ4Qc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the Discriminator\ndef test_D():\n  x = torch.randn(5, 3, 256, 256)\n\n  model = Discriminator(input_channels=3)\n  preds = model(x)\n\n  print(preds.shape)\n\ntest_D()","metadata":{"executionInfo":{"elapsed":862,"status":"ok","timestamp":1688784917787,"user":{"displayName":"Vlad Martinov","userId":"12645653189845640665"},"user_tz":-180},"id":"eJJaYWQ1U75u","outputId":"74071c8c-b673-4445-9ef7-d5140ed27ce4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2><strong>Дополнительный</strong> функционал:</h2>\n<h3>- Буфер</h3>","metadata":{"id":"Zz8k408Dyn96"}},{"cell_type":"code","source":"class CycleBuffer:\n    def __init__(self, max_size=50):\n        assert max_size > 0, \"Max_size is wrong. Be careful!\"\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, img):\n        img_to_return = img\n\n        if len(self.data) < self.max_size:\n            self.data.append(img)\n        else:\n            if random.uniform(0, 1) > 0.5:\n                i = random.randint(0, self.max_size - 1)\n                img_to_return = self.data[i].clone()\n                self.data[i] = img\n\n        return img_to_return","metadata":{"id":"cSudPMm2ynXl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2>Обучение наших моделей на разных задачах</h2>","metadata":{"id":"zYhK1jK2hWw7"}},{"cell_type":"code","source":"disc_M = Discriminator(input_channels=3).to(DEVICE)\ndisc_P = Discriminator(input_channels=3).to(DEVICE)\n\ngen_M = Generator(input_channels=3, num_residual_blocks=9).to(DEVICE)\ngen_P = Generator(input_channels=3, num_residual_blocks=9).to(DEVICE)\n\nopt_disc = optim.Adam(\n    list(disc_M.parameters()) + list(disc_P.parameters()),\n    lr = LEARNING_RATE,\n    betas=(0.5, 0.999),\n)\nopt_gen = optim.Adam(\n    list(gen_M.parameters()) + list(gen_P.parameters()),\n    lr = LEARNING_RATE,\n    betas=(0.5, 0.999),\n)\n\nl1_loss = nn.L1Loss()\nmse_loss = nn.MSELoss()\n\nif LOAD_MODEL:\n  load_checkpoint(\n      CHECKPOINT_GEN_M, gen_M, opt_gen, LEARNING_RATE,\n  )\n  load_checkpoint(\n      CHECKPOINT_GEN_P, gen_P, opt_gen, LEARNING_RATE,\n  )\n\n  load_checkpoint(\n      CHECKPOINT_DISC_M, disc_M, opt_disc, LEARNING_RATE,\n  )\n  load_checkpoint(\n      CHECKPOINT_DISC_P, disc_P, opt_disc, LEARNING_RATE,\n  )\n\ndataset = MonetPhotoSet(\n    root_photo=TRAIN_DIR+\"/trainB\", root_monet=TRAIN_DIR+\"/trainA\", transform=transforms\n)\n\nloader = DataLoader(\n    dataset,\n    batch_size = BATCH_SIZE,\n    shuffle = True,\n    num_workers = NUM_WORKERS,\n    pin_memory = True,\n)\n\ng_scaler=torch.cuda.amp.GradScaler()\nd_scaler=torch.cuda.amp.GradScaler()\n\nbuffer_AB = CycleBuffer(100)\nbuffer_BA = CycleBuffer(100)","metadata":{"executionInfo":{"elapsed":5701,"status":"ok","timestamp":1688784923485,"user":{"displayName":"Vlad Martinov","userId":"12645653189845640665"},"user_tz":-180},"id":"P3vgQxaqmZS5","outputId":"5263600b-ca42-4cb6-f7a1-d6b91c3e7c5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(disc_M, disc_P, gen_M, gen_P, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler, buf_AB, buf_BA):\n  for epoch in range(NUM_EPOCHS):\n    P_reals = 0\n    P_fakes = 0\n\n    loop = tqdm(loader, leave=True)\n\n    for idx, (monet, photo) in enumerate(loop):\n        monet = monet.to(DEVICE)\n        photo = photo.to(DEVICE)\n\n        # Train discriminators P and M\n        with torch.cuda.amp.autocast():\n            # Train discriminators P\n            fake_photo = gen_P(monet)\n            fake_photo_B = buf_BA.push_and_pop(fake_photo)\n\n            D_P_real = disc_P(photo)\n            D_P_fake = disc_P(fake_photo_B.detach())\n\n            P_reals += D_P_real.mean().item()\n            P_fakes += D_P_fake.mean().item()\n\n            D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n            D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_fake))\n\n            D_P_loss = D_P_real_loss + D_P_fake_loss\n\n            # Train discriminators M\n            fake_monet = gen_M(photo)\n            fake_monet_B = buf_AB.push_and_pop(fake_monet)\n\n            D_M_real = disc_M(monet)\n            D_M_fake = disc_M(fake_monet_B.detach())\n\n            D_M_real_loss = mse(D_M_real, torch.ones_like(D_M_real))\n            D_M_fake_loss = mse(D_M_fake, torch.zeros_like(D_M_fake))\n\n            D_M_loss = D_M_real_loss + D_M_fake_loss\n\n            # Add together\n            D_loss = (D_P_loss + D_M_loss) / 2\n\n        opt_disc.zero_grad()\n\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n\n        # Train generators P and M\n        with torch.cuda.amp.autocast():\n            # Adversarial loss\n            D_P_fake = disc_P(fake_photo)\n            D_M_fake = disc_M(fake_monet)\n\n            G_P_loss = mse(D_P_fake, torch.ones_like(D_P_fake))\n            G_M_loss = mse(D_M_fake, torch.ones_like(D_M_fake))\n\n            # Cycle loss\n            cycle_monet = gen_M(fake_monet)\n            cycle_photo = gen_P(fake_photo)\n            cycle_monet_loss = l1(monet, cycle_monet)\n            cycle_photo_loss = l1(photo, cycle_photo)\n\n            # Identity loss (remove these for efficiency if you set lambda_identity=0)\n            identity_monet = gen_M(monet)\n            identity_photo = gen_P(photo)\n            identity_monet_loss = l1(monet, identity_monet)\n            identity_photo_loss = l1(photo, identity_photo)\n\n            # Add togethor\n            G_loss = (\n                G_M_loss +\n                G_P_loss +\n                cycle_monet_loss * LAMBDA_CYCLE +\n                cycle_photo_loss * LAMBDA_CYCLE +\n                identity_photo_loss * LAMBDA_IDENTITY +\n                identity_monet_loss * LAMBDA_IDENTITY\n            )\n\n        opt_gen.zero_grad()\n\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n\n        if idx % 200 == 0:\n            save_image(fake_photo * 0.5 + 0.5, f\"saved_imgs/photo_{idx}.png\")\n            save_image(fake_monet * 0.5 + 0.5, f\"saved_imgs/monet_{idx}.png\")\n\n        loop.set_postfix(P_real=P_reals / (idx + 1), P_fake=P_fakes / (idx + 1))\n\n    if SAVE_MODEL:\n      # Save to local #\n      save_checkpoint(gen_M, opt_gen, filename=CHECKPOINT_GEN_M)\n      save_checkpoint(gen_P, opt_gen, filename=CHECKPOINT_GEN_P)\n      save_checkpoint(disc_M, opt_disc, filename=CHECKPOINT_DISC_M)\n      save_checkpoint(disc_P, opt_disc, filename=CHECKPOINT_DISC_P)\n\n      !zip -r saved_imgs_{epoch}.zip saved_imgs","metadata":{"id":"lafj-93p1ezz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(disc_M, disc_P, gen_M, gen_P, loader, opt_disc, opt_gen, l1_loss, mse_loss, d_scaler, g_scaler, buffer_AB, buffer_BA)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<h2>Используемая литература:</h2>\n<ol>\n  <li><h4><a href=\"https://arxiv.org/pdf/1703.10593.pdf\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.</a></h4></li>\n  <li><h4><a href=\"https://www.youtube.com/watch?v=5jziBapziYE&list=PLhhyoLH6IjfwIp8bZnzX8QR30TRcHO8Va&index=8\">CycleGAN Paper Walkthrough.</a></h4></li>\n  <li><h4><a href=\"https://hannibunny.github.io/mlbook/gan/GAN.html\">HOCHSCHULE DER MEDIEN. Generative Adversarial Nets (GAN)</a></h4></li>\n  <li><h4><a href=\"https://nn.labml.ai/gan/cycle_gan/index.html\">labml.ai. Cycle GAN</a></h4></li>\n  <li><h4><a href=\"https://blog.paperspace.com/unpaired-image-to-image-translations-with-cycle-gans/\">Unpaired Image to Image Translations with Cycle GANs</a></h4></li>\n</ol>\n<hr>","metadata":{"id":"mhatgbVE92ya","outputId":"648ea8c6-8e1c-4f1f-a141-e68ed6dfc067","execution":{"iopub.status.busy":"2023-07-09T10:38:37.196079Z","iopub.execute_input":"2023-07-09T10:38:37.196386Z","iopub.status.idle":"2023-07-09T10:38:37.209367Z","shell.execute_reply.started":"2023-07-09T10:38:37.196357Z","shell.execute_reply":"2023-07-09T10:38:37.207844Z"}}}]}